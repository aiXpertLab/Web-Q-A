{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "from streamlit_extras.add_vertical_space import add_vertical_space\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain, RetrievalQA\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_community.llms       import LlamaCpp, CTransformers\n",
    "from langchain_community.embeddings import LlamaCppEmbeddings, HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader, WebBaseLoader, PyPDFDirectoryLoader\n",
    "\n",
    "SUB_EXT = 'SUB_PDF'\n",
    "SUB_EMB = 'SUB_EMB'\n",
    "EXT = '.pdf'\n",
    "EMB_EXT = '.pkl'\n",
    "FILE_LIST = 'file_name_list.txt'\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def f_scan_directory_for_ext(directory, extension):\n",
    "    return [f for f in os.listdir(directory) if f.endswith(extension)]\n",
    "\n",
    "def f_get_existing_files(file_name_list):\n",
    "    with open(file_name_list, 'r') as file:\n",
    "        return set(file.read().splitlines())\n",
    "\n",
    "def f_update_file_list(file_name_list, new_files):\n",
    "    with open(file_name_list, 'a') as file:\n",
    "        for new_file in new_files:\n",
    "            file.write(new_file + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_create_embedding(new_file_trunk, new_file_pdf_path, file_persistent_dir_path):\n",
    "    # Dummy function - replace with actual embedding logic\n",
    "    print(f\"Creating embedding for {file_persistent_dir_path}\")\n",
    "    pdf_reader = PdfReader(new_file_pdf_path)\n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len)\n",
    "    chunks = text_splitter.split_text(text=text)\n",
    "    # workaround to stupid bug ?!? -> https://github.com/langchain-ai/langchain/issues/2877\n",
    "    new_chunks = [Document(page_content=chunk) for chunk in chunks]\n",
    "    db = Chroma.from_documents(\n",
    "        documents=new_chunks, embedding=embeddings, persist_directory=file_persistent_dir_path)\n",
    "    db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['learn_python_with_jupyter.pdf', 'Managing Your Biological Data with Python.pdf', 'Mastering Machine Learning With scikit-learn.pdf', 'Mastering_Python_50_Specific_Tips_for_Writing_Better.pdf']\n",
      "SUB_EMB\n",
      "Creating embedding for SUB_EMB\\learn_python_with_jupyter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python312\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_validation.py:26: UserWarning: Unsupported Windows version (11). ONNX Runtime supports Windows 10 and above, only.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embedding for SUB_EMB\\Managing Your Biological Data with Python\n",
      "Creating embedding for SUB_EMB\\Mastering Machine Learning With scikit-learn\n",
      "Creating embedding for SUB_EMB\\Mastering_Python_50_Specific_Tips_for_Writing_Better\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Scan the SUB_EXT directory\n",
    "files_in_directory = f_scan_directory_for_ext(SUB_EXT, EXT)\n",
    "print(files_in_directory)\n",
    "\n",
    "# Step 2: Check against the list in file_name_list.txt\n",
    "known_files = f_get_existing_files(FILE_LIST)\n",
    "new_files = [f for f in files_in_directory if f not in known_files]\n",
    "new_files_trunk = [f[:-len(EXT)] for f in new_files ]\n",
    "\n",
    "# Step 3: Process new files\n",
    "# Path to the SUB_EMB directory\n",
    "SUB_EMB_dir = os.path.join(SUB_EMB)\n",
    "print(SUB_EMB_dir)\n",
    "# Iterate over the list of names\n",
    "for name in new_files_trunk:\n",
    "    # Construct the path to the sub-directory\n",
    "    subdir_path = os.path.join(SUB_EMB_dir, name)\n",
    "    # Check if the sub-directory exists\n",
    "    if not os.path.exists(subdir_path):\n",
    "        # If not, create it\n",
    "        os.makedirs(subdir_path)\n",
    "for new_file in new_files:\n",
    "    new_file_trunk = new_file[:-len(EXT)]\n",
    "    #f_create_embedding(new_file_trunk, os.path.join(SUB_EXT, new_file), os.path.join(os.getcwd(), SUB_EMB, new_file_trunk))\n",
    "    f_create_embedding(new_file_trunk, os.path.join(SUB_EXT, new_file), os.path.join(SUB_EMB, new_file_trunk))\n",
    "f_update_file_list(FILE_LIST, new_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize the Streamlit app\n",
    "    st.title('PDF Chatbot App')\n",
    "    # Step 4: Create a Streamlit sidebar with checkboxes\n",
    "    with st.sidebar:\n",
    "        st.title('Step1: Select PDFs')\n",
    "        st.markdown('''\n",
    "        This app is an LLM-powered chatbot allowing\n",
    "        you to load one to all PDFs located in a \n",
    "        dedicated sub-directory into a vector store\n",
    "        so that you can have Q&A-sessions with the\n",
    "        combined selected content.\n",
    "        ''')\n",
    "        add_vertical_space(0)\n",
    "\n",
    "        selected_files = []\n",
    "        for file in files_in_directory:\n",
    "            if st.sidebar.checkbox(file, key=file):\n",
    "                selected_files.append(file)\n",
    "\n",
    "    # Display selected files or perform actions based on selection\n",
    "    st.write('Selected Files (Result of Step1):')\n",
    "    for file in selected_files:\n",
    "        st.write(file)\n",
    "\n",
    "    l_db_pathes_to_load =[\"No confirmed selection yet!\"]\n",
    "    # Button to signal the end of the selection process\n",
    "    if st.button('Step2: Proceed to chat with selected files'):\n",
    "        st.session_state['selected_files'] = selected_files\n",
    "        l_db_pathes_to_load = [os.path.join(SUB_EMB, filename[:-len(EXT)] ) for filename in st.session_state['selected_files']]\n",
    "        for pathname in l_db_pathes_to_load:\n",
    "            st.write(f\"Selected: {pathname}\")\n",
    "\n",
    "    st.header(\"Chat with the PDFs of your choice\")\n",
    "    # Create a new, empty Chroma object to receive input based on the previous document selection\n",
    "    DB_final = Chroma(embedding_function=embeddings)\n",
    "    #loop to load all chorma embedding databases of selected files from disk to vector store\n",
    "    if l_db_pathes_to_load == [\"No confirmed selection yet!\"]:\n",
    "        for pathname in l_db_pathes_to_load:\n",
    "            st.write(pathname)\n",
    "    elif len(l_db_pathes_to_load) == 0:\n",
    "        st.write(\"At least 1 file must be selected\")\n",
    "    else:\n",
    "        for db_path in l_db_pathes_to_load:\n",
    "            # Load the embeddings from the existing vector databases\n",
    "            DB_aux = Chroma(persist_directory=db_path, embedding_function=embeddings)\n",
    "            DB_aux_data = DB_aux._collection.get(include=['documents','metadatas','embeddings'])\n",
    "            DB_final._collection.add(\n",
    "                 embeddings=DB_aux_data['embeddings'],\n",
    "                 metadatas=DB_aux_data['metadatas'],\n",
    "                 documents=DB_aux_data['documents'],\n",
    "                 ids=DB_aux_data['ids'])\n",
    "\n",
    "    # Accept user questions/query via a button-confirmed form:\n",
    "    with st.form(\"query_input\"):\n",
    "        query = st.text_input(\"Step3: Ask questions about the selected PDF file (or enter EXIT to exit):\")\n",
    "        submit_button = st.form_submit_button(\"Submit Query\")\n",
    "\n",
    "    # Initialize session_state if it doesn't exist\n",
    "    if 'chat_history' not in st.session_state:\n",
    "        st.session_state['chat_history'] = []\n",
    "    if query != \"EXIT\":\n",
    "        if submit_button:\n",
    "            st.write(f\"Your query was: {query}\")\n",
    "            retriever = DB_final.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "            chain = ConversationalRetrievalChain.from_llm(llm, retriever, return_source_documents=True)\n",
    "            with get_openai_callback() as cb:\n",
    "                response = chain({'question': query, 'chat_history': st.session_state['chat_history']})\n",
    "                print(cb)\n",
    "            add_vertical_space(1)\n",
    "            st.write(f\"The response: {response['answer']}\")\n",
    "            add_vertical_space(1)\n",
    "            # Update chat_history in session_state\n",
    "            chat_tuple = (query, response['answer'])\n",
    "            st.session_state['chat_history'].append(chat_tuple)\n",
    "    else:\n",
    "        st.warning('You chose to exit the chat.')\n",
    "        st.stop()\n",
    "    # Print chat history to terminal (no GUI)\n",
    "    if 'chat_history' not in st.session_state:\n",
    "        pass\n",
    "    else:\n",
    "        p_chat_history = [entry for entry in st.session_state['chat_history']]\n",
    "        for entry in p_chat_history:\n",
    "            print('--------------')\n",
    "            print(entry)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
